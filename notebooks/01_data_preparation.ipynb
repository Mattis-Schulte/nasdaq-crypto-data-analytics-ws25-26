{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1b49e",
   "metadata": {},
   "source": [
    "# NASDAQ + Crypto price pipeline (2000–2025)\n",
    "\n",
    "This notebook builds clean daily + weekly Open/Close datasets for:\n",
    "- **NASDAQ equities** (from local CSVs, plus split sanity checks)\n",
    "- **Crypto** (from local CSVs, plus yfinance enrichment)\n",
    "- **Optional EURUSD FX** (from yfinance, useful for 2.5 DCA normalization)\n",
    "\n",
    "## What this pipeline does\n",
    "1) Load raw files\n",
    "2) Normalize schema + dates\n",
    "3) Fix splits for NASDAQ (only when unadjusted jump detected)\n",
    "4) Use **yfinance** as a safety net:\n",
    "   - fill missing values\n",
    "   - replace extreme outliers (optional, enabled here)\n",
    "5) Aggregate daily -> weekly\n",
    "6) Save standardized CSVs into `data/processed/`\n",
    "\n",
    "`report_quality(...)` prints a quick data quality summary of each dataset along the way at key steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de75e8d",
   "metadata": {},
   "source": [
    "## Folder layout (expected)\n",
    "\n",
    "We assume this structure:\n",
    "\n",
    "- `data/raw/`\n",
    "  - `nasdaq-daily/`  (CSV per ticker OR CSVs containing a `ticker` column)\n",
    "  - `crypto-daily/`\n",
    "  - `nasdaq_screener.csv`\n",
    "  - `nasdaq_company_addresses.csv`\n",
    "  - `splits_2000_2025.csv`\n",
    "\n",
    "- `data/processed/` *(created automatically)*\n",
    "  - `df_nasdaq_daily.csv`\n",
    "  - `df_nasdaq_weekly.csv`\n",
    "  - `df_crypto_daily.csv`\n",
    "  - `df_crypto_weekly.csv`\n",
    "  - `df_nasdaq_meta.csv`\n",
    "  - optional: `df_eurusd_daily.csv`, `df_eurusd_weekly.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435e8e0",
   "metadata": {},
   "source": [
    "# Pipeline: config\n",
    "Define inputs/outputs, date bounds, and tuning parameters used across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd95c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "RAW, OUT = DATA_DIR / \"raw\", DATA_DIR / \"processed\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "START = pd.Timestamp(\"2000-01-01\")\n",
    "END = pd.Timestamp(\"2025-12-31\")\n",
    "EFFECTIVE_END = min(END, pd.Timestamp.today().normalize())\n",
    "\n",
    "# Relative tolerance for confirming a split jump:\n",
    "# confirmed if |ratio - split_factor| / |split_factor| <= TOL\n",
    "TOL = 0.05\n",
    "\n",
    "# Tukey fences for outlier detection (classic boxplot rule)\n",
    "IQR_K = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381021a",
   "metadata": {},
   "source": [
    "## Config notes\n",
    "\n",
    "- `START` / `END` are the target date bounds for the final outputs.\n",
    "- `EFFECTIVE_END` prevents asking yfinance for future data.\n",
    "- `TOL` controls how strict we are when confirming split jumps.\n",
    "- `IQR_K` controls outlier aggressiveness. Bump to 2.0–3.0 to reduce replacements if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eda30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts a Series to tz-naive pandas timestamps.\n",
    "\n",
    "    :param s: A date-like pandas Series.\n",
    "    :return: A Series of tz-naive timestamps (NaT where parsing fails).\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(s, errors=\"coerce\").dt.tz_localize(None)\n",
    "\n",
    "\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes DataFrame column names (lowercase + stripped).\n",
    "\n",
    "    :param df: Input DataFrame.\n",
    "    :return: The same DataFrame with cleaned column names.\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "def coverage_by_ticker(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes per-ticker coverage stats and calendar-day gaps.\n",
    "\n",
    "    :param df: Price-like DataFrame with columns (ticker, date).\n",
    "    :return: One row per ticker with counts, date span, and gap statistics.\n",
    "    \"\"\"\n",
    "    x = df[[\"ticker\", \"date\"]].dropna().sort_values([\"ticker\", \"date\"]).copy()\n",
    "    x[\"gap_days\"] = x.groupby(\"ticker\")[\"date\"].diff().dt.days\n",
    "    out = x.groupby(\"ticker\", as_index=False).agg(\n",
    "        n=(\"date\", \"size\"),\n",
    "        start=(\"date\", \"min\"),\n",
    "        end=(\"date\", \"max\"),\n",
    "        max_gap_days=(\"gap_days\", \"max\"),\n",
    "        p50_gap_days=(\"gap_days\", \"median\"),\n",
    "    )\n",
    "    out[\"span_days\"] = (out[\"end\"] - out[\"start\"]).dt.days\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840d5b8",
   "metadata": {},
   "source": [
    "## Quick QA\n",
    "\n",
    "Always prints:\n",
    "  - rows, tickers, date range\n",
    "  - duplicates on (ticker, date)\n",
    "  - missing rate for open/close (if present)\n",
    "  - non-positive counts (<=0) for open/close (if present)\n",
    "  - open/close distribution (p01/p50/p99) (if present)\n",
    "  - per-ticker observation count distribution (min/median/max)\n",
    "  - per-ticker gap stats (median of max gaps; global max gap)\n",
    "\n",
    "For diagnostics DataFrames from `merge_prices_with_yf_replace_outliers(...)`, also prints:\n",
    "  - base-missing rate, yfinance-available rate\n",
    "  - outlier counts/rates\n",
    "  - action counts: kept_or_filled / filled_from_yf / replaced_outlier_with_yf\n",
    "  - top tickers by outlier count and by replacement count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_quality(\n",
    "    df: pd.DataFrame,\n",
    "    name: str,\n",
    "    diag: pd.DataFrame | None = None,\n",
    "    top_k: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints a quick quality snapshot for a price-like dataset.\n",
    "\n",
    "    :param df: DataFrame to inspect with columns (typically includes ticker/date/open/close).\n",
    "    :param name: Label shown in the printed report.\n",
    "    :param diag: Optional diagnostics output from merge_prices_with_yf_replace_outliers(...).\n",
    "    :param top_k: Number of tickers to show in the \"top outliers/replacements\" tables.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "\n",
    "    n = len(df)\n",
    "    tickers = df[\"ticker\"].nunique() if \"ticker\" in df else 0\n",
    "    print(\"rows:\", n, \"| tickers:\", tickers)\n",
    "\n",
    "    if \"date\" in df:\n",
    "        print(\"range:\", df[\"date\"].min(), \"->\", df[\"date\"].max())\n",
    "\n",
    "    if {\"ticker\", \"date\"}.issubset(df.columns):\n",
    "        print(\"dup(ticker,date):\", int(df.duplicated([\"ticker\", \"date\"]).sum()))\n",
    "\n",
    "    if {\"open\", \"close\"}.issubset(df.columns):\n",
    "        miss_open = float(df[\"open\"].isna().mean())\n",
    "        miss_close = float(df[\"close\"].isna().mean())\n",
    "        print(\"missing open:\", miss_open, \"| missing close:\", miss_close)\n",
    "\n",
    "        nonpos_open = int((df[\"open\"] <= 0).sum(skipna=True))\n",
    "        nonpos_close = int((df[\"close\"] <= 0).sum(skipna=True))\n",
    "        if nonpos_open or nonpos_close:\n",
    "            print(\"non-positive (<=0) open:\", nonpos_open, \"| close:\", nonpos_close)\n",
    "\n",
    "        def q(s: pd.Series):\n",
    "            s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "            return (np.nan, np.nan, np.nan) if s.empty else (\n",
    "                s.quantile(0.01),\n",
    "                s.quantile(0.50),\n",
    "                s.quantile(0.99),\n",
    "            )\n",
    "\n",
    "        o01, o50, o99 = q(df[\"open\"])\n",
    "        c01, c50, c99 = q(df[\"close\"])\n",
    "        print(\"open  (p01/p50/p99):\", float(o01), float(o50), float(o99))\n",
    "        print(\"close (p01/p50/p99):\", float(c01), float(c50), float(c99))\n",
    "\n",
    "    if {\"ticker\", \"date\"}.issubset(df.columns) and n:\n",
    "        cov = coverage_by_ticker(df)\n",
    "        if not cov.empty:\n",
    "            nn = cov[\"n\"]\n",
    "            print(\"obs/ticker (min/median/max):\", int(nn.min()), int(nn.median()), int(nn.max()))\n",
    "            mg_med = float(cov[\"max_gap_days\"].median(skipna=True))\n",
    "            mg_max = float(cov[\"max_gap_days\"].max(skipna=True))\n",
    "            print(\"max gap days (median/max):\", mg_med, \"/\", mg_max)\n",
    "\n",
    "    if diag is not None and not diag.empty:\n",
    "        print(\"\\n--- diagnostics (merge/fill/outliers) ---\")\n",
    "\n",
    "        nd = len(diag)\n",
    "        base_present = diag[\"base_open\"].notna() & diag[\"base_close\"].notna()\n",
    "        yf_avail = diag[\"yf_open\"].notna() & diag[\"yf_close\"].notna()\n",
    "\n",
    "        n_base_missing = int((~base_present).sum())\n",
    "        n_yf_avail = int(yf_avail.sum())\n",
    "\n",
    "        n_out_any = int(diag[\"outlier_any\"].sum())\n",
    "        n_out_open = int(diag[\"outlier_open\"].sum())\n",
    "        n_out_close = int(diag[\"outlier_close\"].sum())\n",
    "\n",
    "        vc = diag[\"action\"].value_counts()\n",
    "        n_kept = int(vc.get(\"kept_or_filled\", 0))\n",
    "        n_fill = int(vc.get(\"filled_from_yf\", 0))\n",
    "        n_repl = int(vc.get(\"replaced_outlier_with_yf\", 0))\n",
    "\n",
    "        print(\"rows:\", nd, \"| tickers:\", diag[\"ticker\"].nunique())\n",
    "        print(\"base missing rows:\", n_base_missing, f\"({n_base_missing / max(nd,1):.3%})\")\n",
    "        print(\"yfinance available rows:\", n_yf_avail, f\"({n_yf_avail / max(nd,1):.3%})\")\n",
    "        print(\"outliers open/close/any:\", n_out_open, n_out_close, n_out_any, f\"({n_out_any / max(nd,1):.3%})\")\n",
    "        print(\"actions kept/fill/replaced:\", n_kept, n_fill, n_repl)\n",
    "\n",
    "        by = diag.assign(\n",
    "            base_missing=~base_present,\n",
    "            yf_available=yf_avail,\n",
    "            is_filled=diag[\"action\"].eq(\"filled_from_yf\"),\n",
    "            is_replaced=diag[\"action\"].eq(\"replaced_outlier_with_yf\"),\n",
    "        ).groupby(\"ticker\", as_index=False).agg(\n",
    "            n=(\"date\", \"size\"),\n",
    "            n_outlier=(\"outlier_any\", \"sum\"),\n",
    "            n_replaced=(\"is_replaced\", \"sum\"),\n",
    "            n_filled=(\"is_filled\", \"sum\"),\n",
    "            base_missing_rate=(\"base_missing\", \"mean\"),\n",
    "            yf_available_rate=(\"yf_available\", \"mean\"),\n",
    "        )\n",
    "        by[\"outlier_rate\"] = (by[\"n_outlier\"] / by[\"n\"]).fillna(0.0)\n",
    "        by[\"replaced_rate\"] = (by[\"n_replaced\"] / by[\"n\"]).fillna(0.0)\n",
    "\n",
    "        top_out = by.sort_values([\"n_outlier\", \"outlier_rate\"], ascending=False).head(top_k)\n",
    "        top_rep = by.sort_values([\"n_replaced\", \"replaced_rate\"], ascending=False).head(top_k)\n",
    "\n",
    "        print(f\"\\nTop {top_k} tickers by outlier count:\")\n",
    "        print(top_out[\n",
    "            [\"ticker\", \"n\", \"n_outlier\", \"outlier_rate\", \"n_replaced\", \"n_filled\", \"base_missing_rate\"]\n",
    "        ].to_string(index=False))\n",
    "\n",
    "        print(f\"\\nTop {top_k} tickers by replaced-outlier count:\")\n",
    "        print(top_rep[\n",
    "            [\"ticker\", \"n\", \"n_replaced\", \"replaced_rate\", \"n_outlier\", \"yf_available_rate\"]\n",
    "        ].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b46b55",
   "metadata": {},
   "source": [
    "# Outliers: log-returns + Tukey fences\n",
    "\n",
    "Raw prices aren't comparable across tickers (a $5 move means something different for a $2 stock vs $2000 stock); returns normalize that.\n",
    "Additionally, log-returns play nicely with symmetry: if a price goes 3 -> 4 (simple +33.3%) then back 4 -> 3 (simple -25%), the simple returns don't cancel. \n",
    "With log-returns: log(4/3) = -log(3/4) ≈ 0.2877 (≈28.8%), so the two log-returns are equal and opposite and sum to zero.\n",
    "\n",
    "We do outlier detection per ticker, so each asset gets judged relative to its own behavior.\n",
    "`IQR_K` controls how aggressive the Tukey fences are (1.5 is classic boxplot; increase to 2.0–3.0 to reduce replacements if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_log_returns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds per-ticker log prices and log returns for open/close.\n",
    "\n",
    "    :param df: DataFrame with columns (ticker, date, open, close).\n",
    "    :return: A copy of df with added columns log_open/log_close and ret_open/ret_close.\n",
    "    \"\"\"\n",
    "    x = df.sort_values([\"ticker\", \"date\"]).copy()\n",
    "    x[\"log_open\"] = np.log(x[\"open\"].where(x[\"open\"] > 0))\n",
    "    x[\"log_close\"] = np.log(x[\"close\"].where(x[\"close\"] > 0))\n",
    "    x[\"ret_open\"] = x.groupby(\"ticker\")[\"log_open\"].diff()\n",
    "    x[\"ret_close\"] = x.groupby(\"ticker\")[\"log_close\"].diff()\n",
    "    return x\n",
    "\n",
    "\n",
    "def tukey_outlier_mask(df: pd.DataFrame, value_col: str, k: float = IQR_K) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Flags per-ticker outliers using Tukey fences on a numeric column.\n",
    "\n",
    "    :param df: DataFrame that includes (ticker) and the column in value_col.\n",
    "    :param value_col: Name of the numeric column to test (e.g., ret_open).\n",
    "    :param k: IQR multiplier used to form the lower/upper fences.\n",
    "    :return: Boolean Series aligned to df indicating outliers.\n",
    "    \"\"\"\n",
    "    g = df.groupby(\"ticker\")[value_col]\n",
    "    q1 = g.transform(lambda s: s.quantile(0.25))\n",
    "    q3 = g.transform(lambda s: s.quantile(0.75))\n",
    "    iqr = q3 - q1\n",
    "    lo = q1 - k * iqr\n",
    "    hi = q3 + k * iqr\n",
    "    x = df[value_col]\n",
    "    return (x < lo) | (x > hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3b303",
   "metadata": {},
   "source": [
    "# IO and transformations\n",
    "This section contains functions to load/transform:\n",
    "- load raw CSVs\n",
    "- normalize to `(ticker, date, open, close)`\n",
    "- filter to `[START, END]`\n",
    "- turn daily into weekly\n",
    "\n",
    "Weekly aggregation:\n",
    "- weekly open = first open of the week\n",
    "- weekly close = last close of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ccafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_price_dir(price_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads all CSVs in a directory into one long price DataFrame.\n",
    "\n",
    "    :param price_file: Directory containing one CSV per ticker (or CSVs with a ticker column).\n",
    "    :return: DataFrame with columns (ticker, date, open, close) filtered to [START, END].\n",
    "    :raises RuntimeError: If no usable CSVs are found.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for f in sorted(price_file.glob(\"*.csv\")):\n",
    "        d = clean_cols(pd.read_csv(f, keep_default_na=False))\n",
    "        if \"date\" not in d.columns:\n",
    "            continue\n",
    "\n",
    "        if \"ticker\" not in d.columns:\n",
    "            d[\"ticker\"] = f.stem\n",
    "\n",
    "        d[\"ticker\"] = d[\"ticker\"].astype(str).str.strip()\n",
    "        d[\"date\"] = to_date(d[\"date\"])\n",
    "\n",
    "        for c in (\"open\", \"close\"):\n",
    "            if c in d.columns:\n",
    "                d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "                d.loc[d[c] <= 0, c] = np.nan  # treat non-positive as missing\n",
    "            else:\n",
    "                d[c] = np.nan\n",
    "\n",
    "        d = (\n",
    "            d.dropna(subset=[\"ticker\", \"date\"])\n",
    "            .loc[lambda x: x[\"date\"].between(START, END), [\"ticker\", \"date\", \"open\", \"close\"]]\n",
    "            .drop_duplicates([\"ticker\", \"date\"])\n",
    "        )\n",
    "        frames.append(d)\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(f\"No usable CSVs in {price_file}\")\n",
    "\n",
    "    return pd.concat(frames).sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def daily_to_weekly(df: pd.DataFrame, week_ending: str = \"FRI\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates daily open/close prices to weekly open/close.\n",
    "\n",
    "    :param df: Daily DataFrame with columns (ticker, date, open, close).\n",
    "    :param week_ending: Week anchor (e.g., 'FRI' for equities, 'SUN' for crypto).\n",
    "    :return: Weekly DataFrame with (ticker, date, open, close) where date is the week timestamp.\n",
    "    \"\"\"\n",
    "    x = df.sort_values([\"ticker\", \"date\"]).copy()\n",
    "    x[\"date\"] = x[\"date\"].dt.to_period(f\"W-{week_ending}\").dt.to_timestamp()\n",
    "    w = x.groupby([\"ticker\", \"date\"], as_index=False).agg(open=(\"open\", \"first\"), close=(\"close\", \"last\"))\n",
    "    return w.dropna(subset=[\"open\", \"close\"]).sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def load_splits(split_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads split events into (ticker, date, split_factor).\n",
    "\n",
    "    :param split_file: CSV containing split events with at least (date, symbol, stock splits).\n",
    "    :return: DataFrame with columns (ticker, date, split_factor) filtered to [START, END].\n",
    "    \"\"\"\n",
    "    sp = clean_cols(pd.read_csv(split_file, keep_default_na=False))\n",
    "    sp = sp.rename(columns={\"symbol\": \"ticker\", \"stock splits\": \"split_factor\"})\n",
    "    sp[\"date\"] = to_date(sp[\"date\"]).dt.normalize()\n",
    "    sp[\"split_factor\"] = pd.to_numeric(sp[\"split_factor\"], errors=\"coerce\")\n",
    "    sp = sp.dropna(subset=[\"ticker\", \"date\", \"split_factor\"])\n",
    "    sp = sp[sp[\"date\"].between(START, END) & (sp[\"split_factor\"] != 0)]\n",
    "    return sp[[\"ticker\", \"date\", \"split_factor\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543af48",
   "metadata": {},
   "source": [
    "## Split adjustment: only when we see the \"split jump\"\n",
    "\n",
    "Some tickers may already have splits adjusted in the base feed while others may not.\n",
    "We apply splits only if the observed jump around the split date matches the split factor (within `TOL`).\n",
    "This avoids \"double-adjusting\" when the base feed is already clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_splits_if_needed(prices: pd.DataFrame, splits: pd.DataFrame, tol: float = TOL) -> tuple[pd.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Applies split adjustments only when an unadjusted split jump is detected.\n",
    "\n",
    "    :param prices: Daily prices with columns (ticker, date, open, close).\n",
    "    :param splits: Split events with columns (ticker, date, split_factor).\n",
    "    :param tol: Relative tolerance used to confirm the observed jump matches split_factor.\n",
    "    :return: (adjusted_prices_df, n_applied_events).\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    applied = 0\n",
    "\n",
    "    prices = prices[[\"ticker\", \"date\", \"open\", \"close\"]].copy()\n",
    "    splits = splits[[\"ticker\", \"date\", \"split_factor\"]].copy()\n",
    "\n",
    "    for t, dft in prices.groupby(\"ticker\", sort=False):\n",
    "        spt = splits[splits[\"ticker\"] == t].sort_values(\"date\")\n",
    "        if spt.empty or len(dft) < 2:\n",
    "            out.append(dft.sort_values(\"date\"))\n",
    "            continue\n",
    "\n",
    "        dft = dft.sort_values(\"date\").copy()\n",
    "        dft[\"prev_close\"] = dft[\"close\"].shift(1)\n",
    "\n",
    "        left = spt.rename(columns={\"date\": \"split_date\"}).sort_values(\"split_date\")\n",
    "        right = dft[[\"date\", \"close\", \"prev_close\"]].rename(columns={\"date\": \"trade_date\"}).sort_values(\"trade_date\")\n",
    "\n",
    "        chk = pd.merge_asof(\n",
    "            left,\n",
    "            right,\n",
    "            left_on=\"split_date\",\n",
    "            right_on=\"trade_date\",\n",
    "            direction=\"forward\",\n",
    "            allow_exact_matches=True,\n",
    "        )\n",
    "\n",
    "        chk = chk.dropna(subset=[\"trade_date\", \"close\", \"prev_close\", \"split_factor\"])\n",
    "        if chk.empty:\n",
    "            out.append(dft.drop(columns=\"prev_close\"))\n",
    "            continue\n",
    "\n",
    "        ratio = chk[\"prev_close\"] / chk[\"close\"]\n",
    "        ok = ((ratio - chk[\"split_factor\"]).abs() / chk[\"split_factor\"].abs()) <= tol\n",
    "\n",
    "        confirmed = (\n",
    "            chk.loc[ok, [\"trade_date\", \"split_factor\"]]\n",
    "            .groupby(\"trade_date\", as_index=False)[\"split_factor\"]\n",
    "            .prod()\n",
    "            .sort_values(\"trade_date\", ascending=False)\n",
    "        )\n",
    "\n",
    "        if confirmed.empty:\n",
    "            out.append(dft.drop(columns=\"prev_close\"))\n",
    "            continue\n",
    "\n",
    "        for dt, f in confirmed.itertuples(index=False):\n",
    "            dft.loc[dft[\"date\"] < dt, [\"open\", \"close\"]] = dft.loc[dft[\"date\"] < dt, [\"open\", \"close\"]] / float(f)\n",
    "\n",
    "        applied += len(confirmed)\n",
    "        out.append(dft.drop(columns=\"prev_close\"))\n",
    "\n",
    "    prices_adj = pd.concat(out, ignore_index=True).sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "    return prices_adj, applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9de46",
   "metadata": {},
   "source": [
    "# yfinance enrichment\n",
    "\n",
    "We use yfinance as a \"backup feed\":\n",
    "- fill missing values\n",
    "- replace outlier days (optional, but enabled here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yf_fetch_open_close(\n",
    "    tickers: list[str],\n",
    "    start: pd.Timestamp = START,\n",
    "    end: pd.Timestamp = EFFECTIVE_END\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches daily Open/Close from yfinance for the given tickers.\n",
    "\n",
    "    :param tickers: List of yfinance tickers (e.g., ['AAPL', 'BTC-USD']).\n",
    "    :param start: Start date (inclusive).\n",
    "    :param end: End date (inclusive for this pipeline; requested as end+1 day to yfinance).\n",
    "    :return: DataFrame with columns (ticker, date, yf_open, yf_close).\n",
    "    \"\"\"\n",
    "    if not tickers:\n",
    "        return pd.DataFrame(columns=[\"ticker\", \"date\", \"yf_open\", \"yf_close\"])\n",
    "\n",
    "    data = yf.download(\n",
    "        tickers=tickers,\n",
    "        start=start.strftime(\"%Y-%m-%d\"),\n",
    "        end=(end + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\"),  # end exclusive\n",
    "        auto_adjust=True,\n",
    "        group_by=\"ticker\",\n",
    "        progress=True,\n",
    "        threads=False,\n",
    "    )\n",
    "\n",
    "    if data is None or data.empty:\n",
    "        return pd.DataFrame(columns=[\"ticker\", \"date\", \"yf_open\", \"yf_close\"])\n",
    "\n",
    "    syms = sorted(set(data.columns.get_level_values(0)))\n",
    "    frames = [\n",
    "        data[sym][[\"Open\", \"Close\"]]\n",
    "        .rename(columns={\"Open\": \"yf_open\", \"Close\": \"yf_close\"})\n",
    "        .reset_index()\n",
    "        .assign(ticker=sym)\n",
    "        for sym in syms\n",
    "    ]\n",
    "\n",
    "    y = pd.concat(frames, ignore_index=True).rename(columns={\"Date\": \"date\"})\n",
    "    y[\"date\"] = to_date(y[\"date\"])\n",
    "    y[\"yf_open\"] = pd.to_numeric(y[\"yf_open\"], errors=\"coerce\").round(5)\n",
    "    y[\"yf_close\"] = pd.to_numeric(y[\"yf_close\"], errors=\"coerce\").round(5)\n",
    "\n",
    "    return y.dropna(subset=[\"ticker\", \"date\", \"yf_open\", \"yf_close\"])[[\"ticker\", \"date\", \"yf_open\", \"yf_close\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8299b",
   "metadata": {},
   "source": [
    "## Merge logic + outlier replacement\n",
    "\n",
    "We do a full outer merge of base + yfinance, then:\n",
    "1) fill missing base values from yfinance\n",
    "2) detect outliers on base-only values\n",
    "3) if `replace_outliers_yf=True`, overwrite those outlier days with yfinance values (when available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_prices_with_yf_replace_outliers(\n",
    "    base: pd.DataFrame,\n",
    "    yf_df: pd.DataFrame,\n",
    "    replace_outliers_yf: bool = True,\n",
    "    tukey_k: float = IQR_K,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Merges base prices with yfinance, fills missing, and optionally replaces outliers.\n",
    "\n",
    "    :param base: Base DataFrame with columns (ticker, date, open, close).\n",
    "    :param yf_df: yfinance DataFrame with columns (ticker, date, yf_open, yf_close).\n",
    "    :param replace_outliers_yf: If True, replace outlier rows with yfinance when available.\n",
    "    :param tukey_k: IQR multiplier for Tukey outlier fences on per-ticker log-returns.\n",
    "    :return: (final_prices_df, diagnostics_df).\n",
    "    \"\"\"\n",
    "    m = (\n",
    "        base.merge(yf_df, on=[\"ticker\", \"date\"], how=\"outer\")\n",
    "        .sort_values([\"ticker\", \"date\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Preserve originals (and detect outliers on BASE only)\n",
    "    m[\"base_open\"] = m[\"open\"]\n",
    "    m[\"base_close\"] = m[\"close\"]\n",
    "\n",
    "    # Fill missing from yfinance\n",
    "    m[\"open\"] = m[\"open\"].combine_first(m[\"yf_open\"])\n",
    "    m[\"close\"] = m[\"close\"].combine_first(m[\"yf_close\"])\n",
    "\n",
    "    # Outliers (BASE-only)\n",
    "    m[\"outlier_open\"] = False\n",
    "    m[\"outlier_close\"] = False\n",
    "\n",
    "    base_ok = m[\"base_open\"].notna() & m[\"base_close\"].notna()\n",
    "    if base_ok.any():\n",
    "        b = m.loc[base_ok, [\"ticker\", \"date\", \"base_open\", \"base_close\"]].rename(\n",
    "            columns={\"base_open\": \"open\", \"base_close\": \"close\"}\n",
    "        )\n",
    "        b = add_log_returns(b)\n",
    "\n",
    "        b_open = b.dropna(subset=[\"ret_open\"])\n",
    "        b_close = b.dropna(subset=[\"ret_close\"])\n",
    "\n",
    "        if not b_open.empty:\n",
    "            m.loc[b_open.index, \"outlier_open\"] = tukey_outlier_mask(b_open, \"ret_open\", k=tukey_k).values\n",
    "        if not b_close.empty:\n",
    "            m.loc[b_close.index, \"outlier_close\"] = tukey_outlier_mask(b_close, \"ret_close\", k=tukey_k).values\n",
    "\n",
    "    m[\"outlier_any\"] = m[\"outlier_open\"] | m[\"outlier_close\"]\n",
    "\n",
    "    # Actions (for QA)\n",
    "    m[\"action\"] = \"kept_or_filled\"\n",
    "    filled = m[\"base_open\"].isna() & m[\"yf_open\"].notna() & m[\"yf_close\"].notna()\n",
    "    m.loc[filled, \"action\"] = \"filled_from_yf\"\n",
    "\n",
    "    if replace_outliers_yf:\n",
    "        repl = m[\"outlier_any\"] & m[\"yf_open\"].notna() & m[\"yf_close\"].notna()\n",
    "        m.loc[repl, [\"open\", \"close\"]] = np.c_[m.loc[repl, \"yf_open\"], m.loc[repl, \"yf_close\"]]\n",
    "        m.loc[repl, \"action\"] = \"replaced_outlier_with_yf\"\n",
    "    else:\n",
    "        m = m.loc[~m[\"outlier_any\"]].copy()\n",
    "\n",
    "    # Final cleaning\n",
    "    m = (\n",
    "        m.dropna(subset=[\"ticker\", \"date\"])\n",
    "        .loc[m[\"date\"].between(START, END)]\n",
    "        .dropna(subset=[\"open\", \"close\"])\n",
    "        .drop_duplicates([\"ticker\", \"date\"])\n",
    "    )\n",
    "    m[[\"open\", \"close\"]] = m[[\"open\", \"close\"]].round(5)\n",
    "\n",
    "    final = m[[\"ticker\", \"date\", \"open\", \"close\"]].sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    diag = m[\n",
    "        [\n",
    "            \"ticker\", \"date\",\n",
    "            \"base_open\", \"base_close\",\n",
    "            \"yf_open\", \"yf_close\",\n",
    "            \"open\", \"close\",\n",
    "            \"outlier_open\", \"outlier_close\", \"outlier_any\",\n",
    "            \"action\",\n",
    "        ]\n",
    "    ].sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    return final, diag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc00b6b",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "Useful for joining sectors/industries, or using addresses later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta(meta_file: Path, addr_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads NASDAQ metadata and merges it with an address file.\n",
    "\n",
    "    :param meta_file: NASDAQ screener CSV path (expects a symbol column).\n",
    "    :param addr_file: Company address CSV path (expects ticker; address optional).\n",
    "    :return: Metadata DataFrame keyed by ticker.\n",
    "    :raises RuntimeError: If the address file has no ticker column.\n",
    "    \"\"\"\n",
    "    meta = clean_cols(pd.read_csv(meta_file, keep_default_na=False)).rename(columns={\"symbol\": \"ticker\"})\n",
    "    meta = meta.drop_duplicates(\"ticker\")\n",
    "\n",
    "    addr = clean_cols(pd.read_csv(addr_file, keep_default_na=False))\n",
    "    if \"ticker\" not in addr.columns:\n",
    "        raise RuntimeError(\"Address file must have column 'ticker'.\")\n",
    "    if \"address\" not in addr.columns:\n",
    "        addr[\"address\"] = np.nan\n",
    "\n",
    "    addr = addr.drop_duplicates(\"ticker\")[[\"ticker\", \"address\"]]\n",
    "    return meta.merge(addr, on=\"ticker\", how=\"left\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def yf_symbol_crypto(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a crypto ticker to a yfinance USD pair symbol.\n",
    "\n",
    "    :param t: Plain crypto ticker (e.g., 'BTC') or already-qualified (e.g., 'BTC-USD').\n",
    "    :return: yfinance symbol (e.g., 'BTC-USD').\n",
    "    \"\"\"\n",
    "    return t if \"-\" in t else f\"{t}-USD\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c896371",
   "metadata": {},
   "source": [
    "# Run the pipeline (extract -> clean -> enrich -> aggregate)\n",
    "\n",
    "This section produces:\n",
    "- NASDAQ daily/weekly (split check + yfinance fill/replacement)\n",
    "- Crypto daily/weekly (yfinance mapping and fill/replacement)\n",
    "- Optional EURUSD (for FX normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503deb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nasdaq_meta = load_meta(RAW / \"nasdaq_screener.csv\", RAW / \"nasdaq_company_addresses.csv\")\n",
    "\n",
    "# ------------------ NASDAQ ------------------\n",
    "\n",
    "nasdaq = load_price_dir(RAW / \"nasdaq-daily\")\n",
    "splits = load_splits(RAW / \"splits_2000_2025.csv\")\n",
    "\n",
    "nasdaq, n_adj = adjust_splits_if_needed(nasdaq, splits, tol=TOL)\n",
    "report_quality(nasdaq, \"NASDAQ daily (after split-check, before yfinance)\")\n",
    "\n",
    "all_n = sorted(nasdaq[\"ticker\"].unique())\n",
    "yf_n = yf_fetch_open_close(all_n, START, EFFECTIVE_END)\n",
    "\n",
    "df_nasdaq_daily, diag_n = merge_prices_with_yf_replace_outliers(\n",
    "    nasdaq, yf_n, replace_outliers_yf=True, tukey_k=IQR_K\n",
    ")\n",
    "df_nasdaq_weekly = daily_to_weekly(df_nasdaq_daily, week_ending=\"FRI\")\n",
    "\n",
    "# ------------------ Crypto ------------------\n",
    "\n",
    "crypto = load_price_dir(RAW / \"crypto-daily\")\n",
    "report_quality(crypto, \"Crypto daily (before yfinance)\")\n",
    "\n",
    "all_c = sorted(crypto[\"ticker\"].unique())\n",
    "sym_map = {t: yf_symbol_crypto(t) for t in all_c}\n",
    "inv_map = {v: k for k, v in sym_map.items()}\n",
    "\n",
    "yf_c = yf_fetch_open_close(list(sym_map.values()), START, EFFECTIVE_END)\n",
    "if not yf_c.empty:\n",
    "    yf_c[\"ticker\"] = yf_c[\"ticker\"].map(inv_map).fillna(yf_c[\"ticker\"])\n",
    "\n",
    "df_crypto_daily, diag_c = merge_prices_with_yf_replace_outliers(\n",
    "    crypto, yf_c, replace_outliers_yf=True, tukey_k=IQR_K\n",
    ")\n",
    "df_crypto_weekly = daily_to_weekly(df_crypto_daily, week_ending=\"SUN\")\n",
    "\n",
    "# ------------------ EURUSD from yfinance ------------------\n",
    "\n",
    "eurusd_raw = yf_fetch_open_close([\"EURUSD=X\"], START, EFFECTIVE_END)\n",
    "df_eurusd_daily = (\n",
    "    eurusd_raw.rename(columns={\"yf_open\": \"open\", \"yf_close\": \"close\"})\n",
    "    .assign(ticker=\"EURUSD\")[[\"ticker\", \"date\", \"open\", \"close\"]]\n",
    "    .sort_values([\"ticker\", \"date\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_eurusd_weekly = daily_to_weekly(df_eurusd_daily, week_ending=\"FRI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ quality + save ------------------\n",
    "\n",
    "report_quality(df_nasdaq_daily, \"df_nasdaq_daily (final)\", diag=diag_n)\n",
    "report_quality(df_nasdaq_weekly, \"df_nasdaq_weekly (final)\")\n",
    "report_quality(df_crypto_daily, \"df_crypto_daily (final)\", diag=diag_c)\n",
    "report_quality(df_crypto_weekly, \"df_crypto_weekly (final)\")\n",
    "report_quality(df_eurusd_daily, \"df_eurusd_daily (final)\")\n",
    "report_quality(df_eurusd_weekly, \"df_eurusd_weekly (final)\")\n",
    "report_quality(df_nasdaq_meta, \"df_nasdaq_meta (final)\")\n",
    "\n",
    "# Required file names\n",
    "df_nasdaq_daily.to_csv(OUT / \"df_nasdaq_daily.csv\", index=False)\n",
    "df_nasdaq_weekly.to_csv(OUT / \"df_nasdaq_weekly.csv\", index=False)\n",
    "df_crypto_daily.to_csv(OUT / \"df_crypto_daily.csv\", index=False)\n",
    "df_crypto_weekly.to_csv(OUT / \"df_crypto_weekly.csv\", index=False)\n",
    "df_nasdaq_meta.to_csv(OUT / \"df_nasdaq_meta.csv\", index=False)\n",
    "\n",
    "# Optional (FX)\n",
    "df_eurusd_daily.to_csv(OUT / \"df_eurusd_daily.csv\", index=False)\n",
    "df_eurusd_weekly.to_csv(OUT / \"df_eurusd_weekly.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved to:\", OUT.resolve())\n",
    "print(\"Splits adjusted (unadjusted jumps detected):\", n_adj)\n",
    "print(\"EFFECTIVE_END used for yfinance:\", EFFECTIVE_END.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcacd71",
   "metadata": {},
   "source": [
    "## Graphs: yfinance corrections (fills + outlier replacements)\n",
    "\n",
    "These plots are a quick visual QA of the \"merge/fill/outlier-replacement\" step.\n",
    "\n",
    "- **fill** = base `open/close` missing -> filled from yfinance  \n",
    "- **replace** = base present but flagged as log-return outlier -> overwritten with yfinance (when available)\n",
    "\n",
    "### Top 50 tickers by corrections (counts)\n",
    "Stacked bars show, per ticker:\n",
    "- `repl_n`: number of replaced outlier days  \n",
    "- `fill_n`: number of filled missing days  \n",
    "\n",
    "### Per-ticker drilldown (interactive)\n",
    "Dropdown lets you inspect a single ticker:\n",
    "- **final_close** (solid): the saved output  \n",
    "- **base_close** (dotted): original raw series  \n",
    "- **yf_close** (dashed): yfinance reference  \n",
    "- markers highlight **filled** and **replaced** days  \n",
    "\n",
    "### Corrections over time (weekly)\n",
    "Weekly counts of:\n",
    "- `fills` (missing -> yfinance)  \n",
    "- `repls` (outlier -> yfinance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1235e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as w\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fea53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_n = (\n",
    "    diag_n.groupby(\"ticker\", as_index=False)\n",
    "    .agg(\n",
    "        n=(\"date\", \"size\"),\n",
    "        fill_n=(\"action\", lambda s: (s == \"filled_from_yf\").sum()),\n",
    "        repl_n=(\"action\", lambda s: (s == \"replaced_outlier_with_yf\").sum()),\n",
    "    )\n",
    "    .sort_values([\"repl_n\", \"fill_n\"], ascending=False)\n",
    ")\n",
    "\n",
    "fig = px.bar(sum_n.head(50), x=\"ticker\", y=[\"repl_n\", \"fill_n\"],\n",
    "             title=\"Top 50 tickers by corrections (counts)\")\n",
    "fig.update_layout(barmode=\"stack\", xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef6d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = sorted(sum_n[\"ticker\"].tolist())\n",
    "dd = w.Dropdown(options=tickers, description=\"Ticker:\")\n",
    "out = w.Output()\n",
    "\n",
    "def draw_ticker(_=None):\n",
    "    t = dd.value\n",
    "    d_final = df_nasdaq_daily.query(\"ticker == @t\").sort_values(\"date\")\n",
    "    d_diag  = diag_n.query(\"ticker == @t\").sort_values(\"date\")\n",
    "\n",
    "    fill = d_diag[\"action\"].eq(\"filled_from_yf\")\n",
    "    repl = d_diag[\"action\"].eq(\"replaced_outlier_with_yf\")\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scattergl(x=d_final[\"date\"], y=d_final[\"close\"], name=\"final_close\", mode=\"lines\"))\n",
    "    fig.add_trace(go.Scattergl(x=d_diag[\"date\"],  y=d_diag[\"base_close\"], name=\"base_close\", mode=\"lines\",\n",
    "                               line=dict(dash=\"dot\"), opacity=0.6))\n",
    "    fig.add_trace(go.Scattergl(x=d_diag[\"date\"],  y=d_diag[\"yf_close\"], name=\"yf_close\", mode=\"lines\",\n",
    "                               line=dict(dash=\"dash\"), opacity=0.4))\n",
    "    fig.add_trace(go.Scattergl(x=d_diag.loc[fill, \"date\"], y=d_diag.loc[fill, \"close\"],\n",
    "                               name=\"filled_from_yf\", mode=\"markers\",\n",
    "                               marker=dict(size=5, color=\"orange\")))\n",
    "    fig.add_trace(go.Scattergl(x=d_diag.loc[repl, \"date\"], y=d_diag.loc[repl, \"close\"],\n",
    "                               name=\"replaced_outlier_with_yf\", mode=\"markers\",\n",
    "                               marker=dict(size=6, symbol=\"x\", color=\"red\")))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"{t} — close (final/base/yf + corrections)\",\n",
    "        height=550,\n",
    "        legend=dict(orientation=\"h\"),\n",
    "    )\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        fig.show()\n",
    "\n",
    "dd.observe(draw_ticker, names=\"value\")\n",
    "display(w.VBox([dd, out]))\n",
    "draw_ticker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (\n",
    "    diag_n.assign(bucket=diag_n[\"date\"].dt.to_period(\"W\").dt.to_timestamp())\n",
    "    .groupby(\"bucket\", as_index=False)\n",
    "    .agg(\n",
    "        fills=(\"action\", lambda s: (s == \"filled_from_yf\").sum()),\n",
    "        repls=(\"action\", lambda s: (s == \"replaced_outlier_with_yf\").sum()),\n",
    "    )\n",
    ")\n",
    "\n",
    "px.line(t, x=\"bucket\", y=[\"fills\", \"repls\"], title=\"Corrections over time (weekly)\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
